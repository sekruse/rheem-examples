{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Rheem\n",
    "\n",
    "1. Load relevant modules\n",
    "2. Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 13 artifact(s)\n",
      "Adding 4 artifact(s)\n",
      "Adding 0 artifact(s)\n",
      "Adding 1 artifact(s)\n",
      "Adding 0 artifact(s)\n",
      "Adding 37 artifact(s)\n",
      "Adding 2 artifact(s)\n",
      "Adding 30 artifact(s)\n",
      "Adding 3 artifact(s)\n",
      "Adding 84 artifact(s)\n",
      "Adding 0 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcwd\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/basti/Work/Notebooks/boss-2016/examples\"\u001b[0m\n",
       "\u001b[36mrheemVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"0.2.0\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val cwd = \"/Users/basti/Work/Notebooks/boss-2016/examples\"\n",
    "val rheemVersion = \"0.2.0\"\n",
    "\n",
    "// Get the right repositories for SNAPSHOT versions.\n",
    "classpath.addRepository(\"file:///Users/basti/.m2/repository\")\n",
    "classpath.addRepository(\"https://oss.sonatype.org/content/repositories/snapshots\")\n",
    "\n",
    "\n",
    "// Load Rheem's core functionality.\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-core\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-api\" % rheemVersion)\n",
    "\n",
    "// Load Rheem's platform plugins.\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-java\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-spark\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-basic\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-graphchi\" % rheemVersion)\n",
    "classpath.add(\"org.qcri.rheem\" % \"rheem-sqlite3\" % rheemVersion)\n",
    "\n",
    "// Load the platforms themselves.\n",
    "classpath.add(\"org.apache.hadoop\" % \"hadoop-common\" % \"2.2.0\")\n",
    "classpath.add(\"org.apache.hadoop\" % \"hadoop-hdfs\" % \"2.2.0\")\n",
    "classpath.add(\"org.apache.spark\" % \"spark-core_2.11\" % \"1.6.1\")\n",
    "\n",
    "// Load the profiling utility used by Rheem.\n",
    "classpath.add(\"de.hpi.isg\" % \"profiledb-store\" % \"0.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/Users/basti/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-simple/1.7.13/slf4j-simple-1.7.13.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/Users/basti/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/Users/basti/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.qcri.rheem.api._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.basic.data._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.core.api._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.core.util._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.core.function.FunctionDescriptor._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.basic.RheemBasics\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.java.Java\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.spark.Spark\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.qcri.rheem.sqlite3.Sqlite3\u001b[0m\n",
       "\u001b[32mimport \u001b[36mde.hpi.isg.profiledb.store.model._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.JavaConversions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable\u001b[0m\n",
       "\u001b[36mconf\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mqcri\u001b[0m.\u001b[32mrheem\u001b[0m.\u001b[32mcore\u001b[0m.\u001b[32mapi\u001b[0m.\u001b[32mConfiguration\u001b[0m = Configuration[file:///Users/basti/Work/Notebooks/boss-2016/examples/rheem.properties]\n",
       "defined \u001b[32mfunction \u001b[36mprintStats\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Import relevant packages.\n",
    "import org.qcri.rheem.api._\n",
    "import org.qcri.rheem.basic.data._\n",
    "import org.qcri.rheem.core.api._\n",
    "import org.qcri.rheem.core.util._\n",
    "import org.qcri.rheem.core.function.FunctionDescriptor._\n",
    "import org.qcri.rheem.basic.RheemBasics\n",
    "import org.qcri.rheem.java.Java\n",
    "import org.qcri.rheem.spark.Spark\n",
    "import org.qcri.rheem.sqlite3.Sqlite3\n",
    "import de.hpi.isg.profiledb.store.model._\n",
    "\n",
    "import scala.collection.JavaConversions._\n",
    "import scala.collection.mutable\n",
    "\n",
    "// Create a configuration.\n",
    "val conf = new Configuration(s\"file://$cwd/rheem.properties\")\n",
    "\n",
    "// Little utility to show some execution metadata.\n",
    "def printStats(experiment: Experiment) {\n",
    "    def getTime(id: String) = experiment.getMeasurements.find(_.getId == id) match {\n",
    "        case Some(measurement) => measurement.asInstanceOf[TimeMeasurement].getMillis\n",
    "        case _ => 0\n",
    "    }\n",
    "    \n",
    "    val optTime = getTime(\"Optimization\")\n",
    "    val execTime = getTime(\"Execution\")\n",
    "    val loEstTime = getTime(\"Estimate 1 (lower)\")\n",
    "    val hiEstTime = getTime(\"Estimate 1 (upper)\")\n",
    "    import org.qcri.rheem.core.profiling.PartialExecutionMeasurement\n",
    "    val executions = experiment.getMeasurements.toSeq\n",
    "        .filter(_.isInstanceOf[PartialExecutionMeasurement])\n",
    "        .map(_.asInstanceOf[PartialExecutionMeasurement])\n",
    "        .sortBy(_.getId)\n",
    "    \n",
    "    println(\"Statistics\")\n",
    "    println(\"==========\")\n",
    "    println(s\"The optimization took ${Formats.formatDuration(optTime)}.\")\n",
    "    println(s\"The execution took ${Formats.formatDuration(execTime)}, \" + \n",
    "            s\"while Rheem estimated ${Formats.formatDuration(loEstTime)} to ${Formats.formatDuration(hiEstTime)}.\")\n",
    "    for (execution <- executions) {\n",
    "        println(s\"* ${Formats.formatDuration(execution.getExecutionMillis)} \" + \n",
    "                s\"(est. ${execution.getEstimatedExecutionMillis}): ${execution.getOperators.mkString(\", \")}\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/basti/Work/Data/text/odyssey.txt has 7848 different words, for instance:\n",
      "'the' appears 7076 times\n",
      "'and' appears 5406 times\n",
      "'of' appears 3639 times\n",
      "'to' appears 3605 times\n",
      "'a' appears 2052 times\n",
      "'i' appears 2023 times\n",
      "'you' appears 1985 times\n",
      "'in' appears 1930 times\n",
      "'he' appears 1893 times\n",
      "'for' appears 1433 times\n",
      "'his' appears 1343 times\n",
      "'as' appears 1316 times\n",
      "'that' appears 1316 times\n",
      "'it' appears 1284 times\n",
      "'with' appears 1249 times\n",
      "'him' appears 1079 times\n",
      "'was' appears 1065 times\n",
      "'is' appears 971 times\n",
      "'they' appears 971 times\n",
      "'on' appears 934 times\n",
      "'me' appears 921 times\n",
      "'had' appears 891 times\n",
      "'all' appears 887 times\n",
      "'my' appears 884 times\n",
      "'have' appears 880 times\n",
      "'but' appears 876 times\n",
      "'them' appears 795 times\n",
      "'not' appears 795 times\n",
      "'will' appears 737 times\n",
      "'so' appears 714 times\n",
      "'this' appears 693 times\n",
      "'her' appears 673 times\n",
      "'when' appears 667 times\n",
      "'ulysses' appears 655 times\n",
      "'your' appears 651 times\n",
      "'from' appears 647 times\n",
      "'she' appears 633 times\n",
      "'who' appears 632 times\n",
      "'are' appears 618 times\n",
      "'at' appears 602 times\n",
      "'be' appears 581 times\n",
      "'one' appears 573 times\n",
      "'then' appears 570 times\n",
      "'by' appears 566 times\n",
      "'we' appears 558 times\n",
      "'or' appears 525 times\n",
      "'were' appears 519 times\n",
      "'their' appears 494 times\n",
      "'said' appears 483 times\n",
      "'which' appears 431 times\n",
      "\n",
      "Statistics\n",
      "==========\n",
      "The optimization took 0:00:00.624.\n",
      "The execution took 0:00:04.420, while Rheem estimated 0:00:06.157 to 0:00:06.980.\n",
      "* 0:00:00.014 (est. (0:00:00.006 .. 0:00:00.006, p=81.23%)): JavaTextFileSource[Load file], JavaCollect[convert output@JavaTextFileSource[Load file]]\n",
      "* 0:00:00.077 (est. (0:00:00.149 .. 0:00:00.972, p=7.31%)): SparkCollectionSource[convert output@JavaTextFileSource[Load file]], SparkFlatMap[Split words], SparkCollect[convert out@SparkFlatMap[Split words]], JavaFilter[Filter empty words], JavaMap[To lower case, add counter], JavaReduceBy[Add counters]\n",
      "* 0:00:00.003 (est. (0:00:00.002 .. 0:00:00.002, p=76.95%)): JavaLocalCallbackSink[collect()]\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "locally {\n",
    "    val rheemContext = new RheemContext(conf)\n",
    "        .withPlugin(Java.basicPlugin)\n",
    "        .withPlugin(Spark.basicPlugin)\n",
    "    val planBuilder = new PlanBuilder(rheemContext)\n",
    "    val experiment = new Experiment(\"exp01\", new Subject(\"wordcount\", \"v1.0\"))\n",
    "    \n",
    "    val inputUrl = \"file:///Users/basti/Work/Data/text/odyssey.txt\"\n",
    "    import org.qcri.rheem.core.optimizer.ProbabilisticDoubleInterval\n",
    "    val wordsPerLine = new ProbabilisticDoubleInterval(5, 15, 0.9)\n",
    "    \n",
    "    val result = planBuilder\n",
    "      // Do some set up.\n",
    "      .withJobName(s\"WordCount ($inputUrl)\")\n",
    "      .withUdfJarsOf(this.getClass)\n",
    "      .withExperiment(experiment)\n",
    "\n",
    "      // Read the text file.\n",
    "      .readTextFile(inputUrl).withName(\"Load file\")\n",
    "\n",
    "      // Split each line by non-word characters.\n",
    "      .flatMap(_.split(\"\\\\W+\"), selectivity = wordsPerLine).withName(\"Split words\")\n",
    "\n",
    "      // Filter empty tokens.\n",
    "      .filter(_.nonEmpty, selectivity = 0.99).withName(\"Filter empty words\")\n",
    "\n",
    "      // Attach counter to each word.\n",
    "      .map(word => (word.toLowerCase, 1)).withName(\"To lower case, add counter\")\n",
    "\n",
    "      // Sum up counters for every word.\n",
    "      .reduceByKey(_._1, (c1, c2) => (c1._1, c1._2 + c2._2)).withName(\"Add counters\")\n",
    "      .withCardinalityEstimator((in: Long) => math.round(in * 0.01))\n",
    "\n",
    "      // Execute the plan and collect the results.\n",
    "      .collect()\n",
    "\n",
    "    println(s\"$inputUrl has ${result.size} different words, for instance:\")\n",
    "    result.toSeq.sortBy(-_._2).take(50).map(wc => s\"'${wc._1}' appears ${wc._2} times\").foreach(println)\n",
    "\n",
    "    println()\n",
    "    printStats(experiment)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINDY\n",
    "\n",
    "This algorithm loads tuples, or rather cells, from a SQLite database and then detects *inclusion dependencies (INDs)* among the cells' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 INDs:\n",
      "REGION[R_REGIONKEY] is included in NATION[N_REGIONKEY] and NATION[N_NATIONKEY] and CUSTOMER[C_NATIONKEY].\n",
      "NATION[N_NATIONKEY] is included in CUSTOMER[C_NATIONKEY].\n",
      "NATION[N_REGIONKEY] is included in REGION[R_REGIONKEY] and NATION[N_NATIONKEY] and CUSTOMER[C_NATIONKEY].\n",
      "CUSTOMER[C_NATIONKEY] is included in NATION[N_NATIONKEY].\n",
      "\n",
      "Statistics\n",
      "==========\n",
      "The optimization took 0:00:00.882.\n",
      "The execution took 0:00:00.726, while Rheem estimated 0:00:00.386 to 0:00:04.310.\n",
      "* 0:00:00.587 (est. (0:00:00.329 .. 0:00:00.789, p=5.00%)): Sqlite3TableSource[Load REGION], SqlToStream[convert out@Sqlite3TableSource[Load REGION]], JavaFlatMap[Create cells for REGION], Sqlite3TableSource[Load NATION], SqlToStream[convert out@Sqlite3TableSource[Load NATION]], JavaFlatMap[Create cells for NATION], JavaUnionAll[2->1, id=726800f8], Sqlite3TableSource[Load CUSTOMER], Sqlite3Projection[Project CUSTOMER], SqlToStream[convert out@Sqlite3Projection[Project CUSTOMER]], JavaFlatMap[Create cells for CUSTOMER], JavaUnionAll[2->1, id=3650bbe], JavaMap[Prepare cell merging], JavaReduceBy[Merge cells]\n",
      "* 0:00:00.113 (est. (0:00:00.052 .. 0:00:03.289, p=1.01%)): JavaFlatMap[Generate IND candidate sets], JavaReduceBy[Merge IND candidate sets]\n",
      "* 0:00:00.000 (est. (0:00:00.005 .. 0:00:00.232, p=0.91%)): JavaFilter[Filter empty candidate sets], JavaLocalCallbackSink[collect()]\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "locally {\n",
    "    \n",
    "    // Parameters\n",
    "    val jdbcUrl = \"jdbc:sqlite:/Users/basti/Work/Temp/Rheem/tpch.db\"\n",
    "    val schemaDefPath = \"/Users/basti/Work/Temp/Rheem/tpch-schema.txt\"\n",
    "    val tables = Seq((\"REGION\", Seq(\"*\")), (\"NATION\", Seq(\"*\")), (\"CUSTOMER\", Seq(\"C_CUSTKEY\", \"C_NATIONKEY\")))\n",
    "    \n",
    "    \n",
    "    // Define some of the more complex UDFs.\n",
    "    /**\n",
    "    * UDF to create cells from a [[Record]].\n",
    "    *\n",
    "    * @param offset the column ID offset for the input [[Record]]s\n",
    "    */\n",
    "    class CellCreator(val offset: Int) extends SerializableFunction[Record, java.lang.Iterable[(String, Int)]] {\n",
    "\n",
    "    override def apply(record: Record): java.lang.Iterable[(String, Int)] = {\n",
    "      val cells = new java.util.ArrayList[(String, Int)](record.size)\n",
    "      var columnId = offset\n",
    "      for (index <- 0 until record.size) {\n",
    "        cells.add((record.getString(index), columnId))\n",
    "        columnId += 1\n",
    "      }\n",
    "      cells\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * UDF to merge the column IDs of two cells.\n",
    "    */\n",
    "  class CellMerger extends SerializableBinaryOperator[(String, Array[Int])] {\n",
    "\n",
    "    lazy val merger = mutable.Set[Int]()\n",
    "\n",
    "    override def apply(cell1: (String, Array[Int]), cell2: (String, Array[Int])): (String, Array[Int]) = {\n",
    "      merger.clear()\n",
    "      for (columnId <- cell1._2) merger += columnId\n",
    "      for (columnId <- cell2._2) merger += columnId\n",
    "      (cell1._1, merger.toArray)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * UDF to create IND candidates from a cell group.\n",
    "    */\n",
    "  class IndCandidateGenerator extends SerializableFunction[(String, Array[Int]), java.lang.Iterable[(Int, Array[Int])]] {\n",
    "\n",
    "    override def apply(cellGroup: (String, Array[Int])): java.lang.Iterable[(Int, Array[Int])] = {\n",
    "      val columnIds = cellGroup._2\n",
    "      val result = new java.util.ArrayList[(Int, Array[Int])](columnIds.length)\n",
    "      for (i <- columnIds.indices) {\n",
    "        val refColumnIds = new Array[Int](columnIds.length - 1)\n",
    "        java.lang.System.arraycopy(columnIds, 0, refColumnIds, 0, i)\n",
    "        java.lang.System.arraycopy(columnIds, i + 1, refColumnIds, i, refColumnIds.length - i)\n",
    "        result.add((columnIds(i), refColumnIds))\n",
    "      }\n",
    "      result\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * UDF to merge two IND candidates.\n",
    "    */\n",
    "  class IndCandidateMerger extends SerializableBinaryOperator[(Int, Array[Int])] {\n",
    "\n",
    "    lazy val merger = mutable.Set[Int]()\n",
    "\n",
    "    override def apply(indc1: (Int, Array[Int]), indc2: (Int, Array[Int])): (Int, Array[Int]) = {\n",
    "      merger.clear()\n",
    "      for (columnId <- indc1._2) merger += columnId\n",
    "      (indc1._1, indc2._2.filter(merger.contains))\n",
    "    }\n",
    "\n",
    "  }\n",
    "    \n",
    "    // Load the schema definition.\n",
    "    val tableRegex = \"\"\"(\\w+)\\[((?:\\w+(?:,\\w+)*)|(?:\\*))\\]\"\"\".r\n",
    "    val schema = scala.io.Source.fromFile(schemaDefPath).getLines().map(_ match {\n",
    "        case tableRegex(table, columns) => (table, columns.split(',').toSeq)\n",
    "        case other: String => sys.error(s\"Illegal table specifier: $other\")\n",
    "    }).toMap\n",
    "    \n",
    "    // Prepare the RheemContext.\n",
    "    val rheemContext = new RheemContext(conf)\n",
    "        .withPlugin(Java.basicPlugin)\n",
    "        .withPlugin(Spark.basicPlugin)\n",
    "        .withPlugin(Sqlite3.plugin)\n",
    "    rheemContext.getConfiguration.setProperty(\"rheem.sqlite3.jdbc.url\", jdbcUrl)\n",
    "    val experiment = new Experiment(\"exp02\", new Subject(\"sindy\", \"v1.0\"))\n",
    "    val planBuilder = new PlanBuilder(rheemContext).withExperiment(experiment)\n",
    "\n",
    "    // Create cells from the various tables.\n",
    "    var offset = 0\n",
    "    val columnsById = mutable.Map[Int, String]()\n",
    "    val allCells = tables\n",
    "      .map { case (table, columns) =>\n",
    "        // Handle the special case where columns == \"*\".\n",
    "        var resolvedColumns = if (columns == Seq(\"*\")) schema(table) else columns\n",
    "\n",
    "        // Read the cells from the specified table/columns.\n",
    "        var records = planBuilder.readTable(new org.qcri.rheem.sqlite3.operators.Sqlite3TableSource(table, schema(table): _*)).withName(s\"Load $table\")\n",
    "\n",
    "        // If requested, project to the relevant fields.\n",
    "        if (resolvedColumns.toSet != schema(table).toSet)\n",
    "          records = records.projectRecords(resolvedColumns).withName(s\"Project $table\")\n",
    "\n",
    "        // Create the cells, finally.\n",
    "        val cells = records.flatMapJava(new CellCreator(offset), selectivity = resolvedColumns.size.toDouble).withName(s\"Create cells for $table\")\n",
    "\n",
    "        // Maintain some helper data structures.\n",
    "        resolvedColumns.zipWithIndex.foreach { case (column, index) => columnsById(offset + index) = s\"$table[$column]\" }\n",
    "        offset += resolvedColumns.size\n",
    "\n",
    "        cells\n",
    "      }\n",
    "      .reduce(_ union _)\n",
    "\n",
    "    // Do the rest of the SINDY logic on the cells.\n",
    "    val rawInds = allCells\n",
    "      .map(cell => (cell._1, Array(cell._2))).withName(\"Prepare cell merging\")\n",
    "      .reduceByKeyJava(toSerializableFunction(_._1), new CellMerger).withName(\"Merge cells\")\n",
    "      .flatMapJava(new IndCandidateGenerator).withName(\"Generate IND candidate sets\")\n",
    "      .reduceByKeyJava(toSerializableFunction(_._1), new IndCandidateMerger).withName(\"Merge IND candidate sets\")\n",
    "      .filter(_._2.length > 0).withName(\"Filter empty candidate sets\")\n",
    "      .collect()\n",
    "\n",
    "    // Make the results readable.\n",
    "    val inds = rawInds.map {\n",
    "      case (dep, refs) => (s\"${columnsById(dep)}\", refs.map(columnsById).toSeq)\n",
    "    }\n",
    "    \n",
    "    println(s\"Found ${inds.map(_._2.length).sum} INDs:\")\n",
    "    inds.foreach { case (dep, refs) => println(s\"$dep is included in ${refs.mkString(\" and \")}.\")}\n",
    "    \n",
    "    println\n",
    "    printStats(experiment)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank\n",
    "\n",
    "The following Rheem code loads an RDF triple file, creates a graph from it by encoding vertices as IDs and edges as pairs of vertex IDs. The resulting graph is fed into a PageRank and the page ranks are eventually made readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate page ranks for 659133 vertices:\n",
      "<http://dbpedia.org/resource/United_States> has a page rank of 0.0010866255.\n",
      "<http://dbpedia.org/resource/France> has a page rank of 5.982427E-4.\n",
      "<http://dbpedia.org/resource/Germany> has a page rank of 5.957829E-4.\n",
      "<http://dbpedia.org/resource/World_War_II> has a page rank of 5.1534176E-4.\n",
      "<http://dbpedia.org/resource/Category:American_people> has a page rank of 4.3901728E-4.\n",
      "<http://dbpedia.org/resource/Italy> has a page rank of 3.9436592E-4.\n",
      "<http://dbpedia.org/resource/English_language> has a page rank of 3.8891463E-4.\n",
      "<http://dbpedia.org/resource/United_Kingdom> has a page rank of 3.4536282E-4.\n",
      "<http://dbpedia.org/resource/Switzerland> has a page rank of 3.365571E-4.\n",
      "<http://dbpedia.org/resource/World_War_I> has a page rank of 3.1882824E-4.\n",
      "<http://dbpedia.org/resource/Paris> has a page rank of 3.0596784E-4.\n",
      "<http://dbpedia.org/resource/New_York_City> has a page rank of 3.0100156E-4.\n",
      "<http://dbpedia.org/resource/Category:German_people> has a page rank of 2.9207076E-4.\n",
      "<http://dbpedia.org/resource/Austria> has a page rank of 2.775387E-4.\n",
      "<http://dbpedia.org/resource/London> has a page rank of 2.740376E-4.\n",
      "<http://dbpedia.org/resource/Canada> has a page rank of 2.7197317E-4.\n",
      "<http://dbpedia.org/resource/Berlin> has a page rank of 2.7118597E-4.\n",
      "<http://dbpedia.org/resource/Russia> has a page rank of 2.6762547E-4.\n",
      "<http://dbpedia.org/resource/Association_football> has a page rank of 2.5651415E-4.\n",
      "<http://dbpedia.org/resource/England> has a page rank of 2.4606974E-4.\n",
      "<http://dbpedia.org/resource/Spain> has a page rank of 2.4557926E-4.\n",
      "<http://dbpedia.org/resource/Latin> has a page rank of 2.3925334E-4.\n",
      "<http://dbpedia.org/resource/Category:Writers> has a page rank of 2.3755526E-4.\n",
      "<http://dbpedia.org/resource/California> has a page rank of 2.2431264E-4.\n",
      "<http://dbpedia.org/resource/Japan> has a page rank of 2.2277278E-4.\n",
      "<http://dbpedia.org/resource/German_language> has a page rank of 2.0967482E-4.\n",
      "<http://dbpedia.org/resource/Poland> has a page rank of 2.0882959E-4.\n",
      "<http://dbpedia.org/resource/India> has a page rank of 1.9453221E-4.\n",
      "<http://dbpedia.org/resource/French_language> has a page rank of 1.920665E-4.\n",
      "<http://dbpedia.org/resource/U.S._state> has a page rank of 1.9109814E-4.\n",
      "<http://dbpedia.org/resource/Netherlands> has a page rank of 1.8954289E-4.\n",
      "<http://dbpedia.org/resource/Soviet_Union> has a page rank of 1.8663102E-4.\n",
      "<http://dbpedia.org/resource/Sweden> has a page rank of 1.794886E-4.\n",
      "<http://dbpedia.org/resource/Category:Actors> has a page rank of 1.7876958E-4.\n",
      "<http://dbpedia.org/resource/Actor> has a page rank of 1.7705486E-4.\n",
      "<http://dbpedia.org/resource/Politician> has a page rank of 1.7508295E-4.\n",
      "<http://dbpedia.org/resource/Catholic_Church> has a page rank of 1.6836215E-4.\n",
      "<http://dbpedia.org/resource/Australia> has a page rank of 1.6723899E-4.\n",
      "<http://dbpedia.org/resource/Europe> has a page rank of 1.6581088E-4.\n",
      "<http://dbpedia.org/resource/Person> has a page rank of 1.5985842E-4.\n",
      "<http://dbpedia.org/resource/China> has a page rank of 1.5854985E-4.\n",
      "<http://dbpedia.org/resource/Category:British_people> has a page rank of 1.565515E-4.\n",
      "<http://dbpedia.org/resource/Vienna> has a page rank of 1.5565596E-4.\n",
      "<http://dbpedia.org/resource/Rome> has a page rank of 1.4784158E-4.\n",
      "<http://dbpedia.org/resource/Denmark> has a page rank of 1.4706489E-4.\n",
      "<http://dbpedia.org/resource/Brazil> has a page rank of 1.4459356E-4.\n",
      "<http://dbpedia.org/resource/United_States_dollar> has a page rank of 1.4433841E-4.\n",
      "<http://dbpedia.org/resource/Ancient_Rome> has a page rank of 1.4196007E-4.\n",
      "<http://dbpedia.org/resource/Japanese_writing_system> has a page rank of 1.3660626E-4.\n",
      "<http://dbpedia.org/resource/Turkey> has a page rank of 1.3485778E-4.\n",
      "\n",
      "Statistics\n",
      "==========\n",
      "The optimization took 0:00:00.133.\n",
      "The execution took 0:00:27.666, while Rheem estimated 0:00:47.708 to 2:18:42.213.\n",
      "* 0:00:02.374 (est. (0:00:00.535 .. 0:00:03.224, p=18.05%)): JavaTextFileSource[Load file], JavaFilter[Filter comments], JavaMap[Parse triples], JavaMap[Discard predicate], JavaCollect[convert out@JavaMap[Discard predicate]]\n",
      "* 0:00:02.660 (est. (0:00:00.374 .. 0:01:26.089, p=3.58%)): JavaFlatMap[Extract vertices], JavaDistinct[Distinct vertices], JavaMap[Add vertex IDs], JavaCollect[convert out@JavaMap[Add vertex IDs]]\n",
      "* 0:00:05.283 (est. (0:00:04.590 .. 0:18:42.146, p=0.22%)): JavaJoin[Join source vertex IDs], JavaMap[Set source vertex ID], JavaJoin[Join target vertex IDs], JavaMap[Set target vertex ID], JavaMap[PageRank (forward)], JavaCollect[convert out@JavaMap[PageRank (forward)]]\n",
      "* 0:00:00.322 (est. (0:00:03.063 .. 0:03:56.576, p=0.11%)): JavaMap[PageRank (prepare adjacencies)], JavaReduceBy[PageRank (create adjacencies)]\n",
      "* 0:00:00.901 (est. (0:00:05.091 .. 0:20:20.765, p=0.45%)): JavaFlatMap[PageRank (extract vertices)], JavaDistinct[PageRank (distinct vertices)], JavaCollect[convert out@JavaDistinct[PageRank (distinct vertices)]]\n",
      "* 0:00:00.001 (est. (0:00:00.002 .. 0:00:00.014, p=6.73%)): JavaCount[PageRank (count vertices)]\n",
      "* 0:00:00.001 (est. (0:00:00.002 .. 0:00:00.002, p=0.50%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:01.826 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (initialize ranks)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.12%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:02.252 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (damping)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.03%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:00.982 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (damping)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.01%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:01.008 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (damping)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.00%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:01.830 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (damping)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.00%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:00.921 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (damping)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.00%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:01.303 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (damping)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.00%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:01.138 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (damping)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.00%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:01.158 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (damping)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.00%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:01.017 (est. (0:00:03.394 .. 0:08:58.326, p=0.00%)): JavaMap[PageRank (damping)], JavaJoin[PageRank (join adjacencies and ranks)], JavaFlatMap[PageRank (create partial ranks)], JavaReduceBy[PageRank (sum partial ranks)]\n",
      "* 0:00:00.000 (est. (0:00:00.002 .. 0:00:00.002, p=0.00%)): JavaRepeat[PageRank (loop head)]\n",
      "* 0:00:00.071 (est. (0:00:00.005 .. 0:00:00.178, p=0.00%)): JavaMap[PageRank (damping)]\n",
      "* 0:00:00.271 (est. (0:00:00.086 .. 0:04:29.939, p=0.02%)): JavaJoin[Join page ranks with vertex IDs], JavaMap[Make page ranks readable], JavaLocalCallbackSink[collect()]\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "locally {\n",
    "    import org.qcri.rheem.api.graph._\n",
    "    \n",
    "    val inputUrl = \"file:///Users/basti/Work/Data/rdf/dbpedia/page-links-en-uris_de.sample_5pc.nt\"\n",
    "    val numIterations = 10\n",
    "    \n",
    "    // Initialize.\n",
    "    val rheemCtx = new RheemContext(conf)\n",
    "        .withPlugin(Java.basicPlugin)\n",
    "        .withPlugin(RheemBasics.graphPlugin)\n",
    "    val experiment = new Experiment(\"exp03\", new Subject(\"pagerank\", \"v1.0\"))\n",
    "    val planBuilder = new PlanBuilder(rheemCtx)\n",
    "      .withJobName(s\"PageRank ($inputUrl, $numIterations iterations)\")\n",
    "      .withUdfJarsOf(this.getClass)\n",
    "      .withExperiment(experiment)\n",
    "\n",
    "    // Read and parse the input file.\n",
    "    val edges = planBuilder\n",
    "      .readTextFile(inputUrl).withName(\"Load file\")\n",
    "      .filter(!_.startsWith(\"#\"), selectivity = 1.0).withName(\"Filter comments\")\n",
    "      .map { raw =>\n",
    "            // Find the first two spaces: Odds are that these are separate subject, predicated and object.\n",
    "            val firstSpacePos = raw.indexOf(' ')\n",
    "            val secondSpacePos = raw.indexOf(' ', firstSpacePos + 1)\n",
    "\n",
    "            // Find the end position.\n",
    "            var stopPos = raw.lastIndexOf('.')\n",
    "            while (raw.charAt(stopPos - 1) == ' ') stopPos -= 1\n",
    "\n",
    "            (raw.substring(0, firstSpacePos),\n",
    "             raw.substring(firstSpacePos + 1, secondSpacePos),\n",
    "             raw.substring(secondSpacePos + 1, stopPos))\n",
    "      }.withName(\"Parse triples\")\n",
    "      .map { case (s, p, o) => (s, o) }.withName(\"Discard predicate\")\n",
    "\n",
    "    // Create vertex IDs.\n",
    "    val vertexIds = edges\n",
    "      .flatMap(edge => Seq(edge._1, edge._2)).withName(\"Extract vertices\")\n",
    "      .distinct.withName(\"Distinct vertices\")\n",
    "      .zipWithId.withName(\"Add vertex IDs\")\n",
    "\n",
    "    // Encode the edges with the vertex IDs\n",
    "    type VertexId = org.qcri.rheem.basic.data.Tuple2[Vertex, String]\n",
    "    val idEdges = edges\n",
    "      .join[VertexId, String](_._1, vertexIds, _.field1).withName(\"Join source vertex IDs\")\n",
    "      .map { linkAndVertexId =>\n",
    "        (linkAndVertexId.field1.field0, linkAndVertexId.field0._2)\n",
    "      }.withName(\"Set source vertex ID\")\n",
    "      .join[VertexId, String](_._2, vertexIds, _.field1).withName(\"Join target vertex IDs\")\n",
    "      .map(linkAndVertexId => new Edge(linkAndVertexId.field0._1, linkAndVertexId.field1.field0)).withName(\"Set target vertex ID\")\n",
    "\n",
    "    // Run the PageRank.\n",
    "    // Note: org.qcri.rheem.api.graph._ must be imported for this to work.\n",
    "    val pageRanks = idEdges.pageRank(numIterations)\n",
    "\n",
    "    // Make the page ranks readable.\n",
    "    val result = pageRanks\n",
    "      .join[VertexId, Long](_.field0, vertexIds, _.field0).withName(\"Join page ranks with vertex IDs\")\n",
    "      .map(joinTuple => (joinTuple.field1.field1, joinTuple.field0.field1)).withName(\"Make page ranks readable\")\n",
    "      .collect()\n",
    "    \n",
    "    println(s\"Calculate page ranks for ${result.size} vertices:\")\n",
    "    result.toSeq.sortBy(-_._2).take(50).foreach(pr => println(s\"${pr._1} has a page rank of ${pr._2}.\"))\n",
    "    println\n",
    "    printStats(experiment)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}